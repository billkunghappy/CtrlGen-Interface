{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import NoRepeatNGramLogitsProcessor, RepetitionPenaltyLogitsProcessor\n",
    "from transformers import BeamSearchScorer, LogitsProcessorList, LogitsProcessor, StoppingCriteriaList, StoppingCriteria, MaxLengthCriteria\n",
    "\n",
    "from HMM.hmm_model import *\n",
    "from HMM.DFA_model import *\n",
    "from model_utils import (\n",
    "    encode_with_messages_format,\n",
    "    get_prefix_suffix_tokens_for_HMM,\n",
    "    get_sequence_scores,\n",
    "    ConstraintLogitsProcessor\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init():\n",
    "    global device\n",
    "    global CUDA_CORE\n",
    "    global args\n",
    "\n",
    "    arg_parser = argparse.ArgumentParser()\n",
    "    arg_parser.add_argument('--device', default='cuda', type=str)\n",
    "    arg_parser.add_argument('--cuda_core', default='1', type=str)\n",
    "    arg_parser.add_argument('--hmm_batch_size', default=256, type=int)\n",
    "    arg_parser.add_argument('--hmm_model_path', default=None, type=str)\n",
    "    arg_parser.add_argument('--llama_model_path', default='gpt2', type=str)\n",
    "    arg_parser.add_argument('--do_beam_search', action='store_true')\n",
    "    arg_parser.add_argument('--debug', action='store_true')\n",
    "\n",
    "    args = arg_parser.parse_args([\n",
    "        # \"--hmm_model_path\", \"/local1/hzhang19/matcha/models/hmm_llama-story-pretrain-finetune_4096_64/checkpoint-60.weight.th\",\n",
    "        \"--hmm_model_path\", \"/local1/hzhang19/matcha/models/hmm_llama-story-pretrain-finetune_32768_64/checkpoint-50.weight.th\",\n",
    "        # \"--hmm_model_path\", \"/local1/hzhang19/matcha/models/hmm_llama-story-finetune_32768_64/checkpoint-60.weight.th\",\n",
    "        \"--llama_model_path\", \"/local1/ponienkung/CtrlGen/output/NewFinetunePretrained_Filtered_StoryPretrain-TULU-LLAMA2\",\n",
    "        # \"--llama_model_path\", \"/local1/ponienkung/CtrlGen/output/NewFinetuneTULU_Filtered_StoryPretrain-TULU-LLAMA2\",\n",
    "        # Old ones\n",
    "        # \"--hmm_model_path\", \"/local1/hzhang19/matcha/models/hmm_llama-story-cont-para_32768_64/checkpoint-90.weight.th\",\n",
    "        # \"--llama_model_path\", \"/local1/ponienkung/CtrlGen/output/NewFinetune_cont_para_2K_8K_2K_StoryPretrain-TULU-LLAMA2\",\n",
    "        \"--hmm_batch_size\",  \"2\",\n",
    "        \"--cuda_core\", \"7\",\n",
    "    ])\n",
    "    device = args.device\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_core\n",
    "    torch.cuda.set_device(int(args.cuda_core))\n",
    "\n",
    "def load_models():\n",
    "    global tokenizer\n",
    "    global llama_model\n",
    "    global hmm_model\n",
    "    global keyphrase_builder\n",
    "    global end_sentence_builder\n",
    "    global word_count_builder\n",
    "    global trivial_builder\n",
    "    global eos_builder\n",
    "    try:\n",
    "        print(f'loading llama2 from {args.llama_model_path} ...')\n",
    "        llama_model = LlamaForCausalLM.from_pretrained(args.llama_model_path).to(device)\n",
    "        llama_model.half()\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(args.llama_model_path)    \n",
    "\n",
    "        print(f'loading hmm from {args.hmm_model_path} ...')\n",
    "        hmm_model = HMM(args.hmm_model_path)\n",
    "        hmm_model.to(device)\n",
    "\n",
    "        print(f'constructing DFA builders ...')\n",
    "        keyphrase_builder = KeyphraseBuilder(tokenizer, 32000)\n",
    "        end_sentence_builder = EndSentenceBuilder(tokenizer, 32000)\n",
    "        word_count_builder = WordCountBuilder(tokenizer, 32000)\n",
    "        trivial_builder = TrivialBuilder(tokenizer, 32000)\n",
    "        eos_builder = EOSBuilder(tokenizer, 32000)\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot Load args.model {args.model_name_or_path} because of the following exception:\\n {e}\")\n",
    "        print(\"Exit the process...\")\n",
    "        exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading llama2 from /local1/ponienkung/CtrlGen/output/NewFinetunePretrained_Filtered_StoryPretrain-TULU-LLAMA2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hmm from /local1/hzhang19/matcha/models/hmm_llama-story-pretrain-finetune_32768_64/checkpoint-50.weight.th ...\n",
      "constructing DFA builders ...\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_json = {\n",
    "#     'Prefix': 'Once upon a time there was an old mother pig who had one hundred little pigs and not enough food to feed them. So when they were old enough, ', \n",
    "#     'Suffix': 'a story about the 92nd little pig.', \n",
    "#     'Instruct': '', \n",
    "#     'Prior': '', \n",
    "#     'Operation': 'Continuation', \n",
    "#     'temperature': 0.95, \n",
    "#     'num_return_sequences': 10, \n",
    "#     'num_beams': 10, \n",
    "#     'no_repeat_ngram_size': 2, \n",
    "#     'top_p': 1.0\n",
    "#     'max_tokens': 50, --> token_contraints [min, max]\n",
    "# }\n",
    "# Need to add constraints here in the input_json\n",
    "# 'word_contraints':\n",
    "# 'keyword_constraints':\n",
    "\n",
    "def prompt(input_json):\n",
    "    # Get the text and operation\n",
    "    Prefix, Prior, Suffix, Instruct, Operation = input_json['Prefix'], input_json['Prior'], input_json['Suffix'], input_json['Instruct'], input_json['Operation']\n",
    "    Prefix = Prefix.rstrip(\" \")\n",
    "    # Get the constraints\n",
    "    token_constraint, word_contraint, keyword_constraint = input_json[\"token_constraint\"], input_json[\"word_contraint\"], input_json[\"keyword_constraint\"]\n",
    "    max_tokens = token_constraint[1]\n",
    "    # Get generation config\n",
    "    temperature = input_json['temperature']\n",
    "    num_return_sequences = input_json['num_return_sequences']\n",
    "    num_beams = input_json['num_beams']\n",
    "    no_repeat_ngram_size = input_json['no_repeat_ngram_size']\n",
    "    top_p = input_json['top_p']\n",
    "    \n",
    "    # TODO\n",
    "    if word_contraint != []:\n",
    "        max_tokens = max(max_tokens, int(1.5 * word_contraint[1]))\n",
    "    \n",
    "        # max_tokens = max_tokens\n",
    "    # Get prefix, suffix tokens for HMM\n",
    "    prefix_tokens, suffix_tokens = get_prefix_suffix_tokens_for_HMM(Prefix, Suffix, tokenizer)\n",
    "\n",
    "    # Construct DFA graph\n",
    "    # dfa_graphs = [eos_builder.build()]\n",
    "    dfa_graphs = []\n",
    "    has_constraints = False\n",
    "    if keyword_constraint != []:\n",
    "        print(\"Build Keyword\")\n",
    "        dfa_graphs.append(keyphrase_builder.build(keyword_constraint))\n",
    "        has_constraints = True\n",
    "    if word_contraint != []:\n",
    "        print(\"Build Word Length\")\n",
    "        dfa_graphs.append(word_count_builder.build(word_contraint[0], word_contraint[1]))\n",
    "        has_constraints = True\n",
    "    # MODIFIED: Comment out this\n",
    "    if (Suffix == '') and has_constraints:\n",
    "        dfa_graphs.append(end_sentence_builder.build())\n",
    "    if dfa_graphs != []:\n",
    "        dfa_model = DFAModel(DFA_prod(dfa_graphs, mode='intersection'))\n",
    "    else:\n",
    "        dfa_model = DFAModel(trivial_builder.build())\n",
    "    if (not has_constraints) and Suffix == '': # Freeform continuation\n",
    "        USE_HMM = False\n",
    "    else:\n",
    "        USE_HMM = True\n",
    "\n",
    "\n",
    "    # Get input_ids\n",
    "    prompt_tokens = encode_with_messages_format(\n",
    "        Prefix = Prefix,\n",
    "        SoftControl = Instruct, \n",
    "        Prior = Prior,\n",
    "        Suffix = Suffix,\n",
    "        tokenizer = tokenizer, \n",
    "        operation = Operation\n",
    "        )\n",
    "    \n",
    "    input_ids = torch.tensor([prompt_tokens] * num_beams, device=device)\n",
    "    max_length = len(prompt_tokens) + max_tokens\n",
    "    print(f\"max_length: {max_length}, max_tokens: {max_tokens}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        past_key_values = llama_model(input_ids[:, :-1], return_dict=True).past_key_values\n",
    "\n",
    "        model_kwargs = {\n",
    "            'past_key_values': past_key_values,\n",
    "        }\n",
    "\n",
    "        hmm_model.initialize_cache(prefix_tokens, suffix_tokens,\n",
    "            [(1, max_tokens)], dfa_model)\n",
    "\n",
    "        hmm_config = {\n",
    "            'hmm_prompt_len': len(prompt_tokens),\n",
    "            'hmm_prefix': prefix_tokens,\n",
    "            'hmm_suffix': suffix_tokens,\n",
    "            'hmm_generation_offset': len(prefix_tokens),\n",
    "            'hmm_min_tokens': 1,\n",
    "            'hmm_max_tokens': max_tokens,\n",
    "            'hmm_batch_size': args.hmm_batch_size,\n",
    "        }\n",
    "\n",
    "        # Init logits processors\n",
    "        stopping_criteria = StoppingCriteriaList([\n",
    "            MaxLengthCriteria(max_length=max_length)])\n",
    "        logits_processor = LogitsProcessorList([\n",
    "            ConstraintLogitsProcessor(hmm_model, hmm_config, temperature=temperature)] if USE_HMM else [])\n",
    "        if no_repeat_ngram_size > 0:\n",
    "            logits_processor.append(NoRepeatNGramLogitsProcessor(no_repeat_ngram_size))\n",
    "        print(\"Logits Processor: \", logits_processor)\n",
    "        # If use beamsearch\n",
    "        if args.do_beam_search:\n",
    "            print(\"Do beamsearch\")\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=1,\n",
    "                num_beams=num_beams,\n",
    "                num_beam_hyps_to_keep=num_beams,\n",
    "                device=llama_model.device,\n",
    "            )\n",
    "            outputs= llama_model.beam_search(\n",
    "                input_ids,\n",
    "                beam_scorer,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                **model_kwargs\n",
    "            )\n",
    "        else:\n",
    "            outputs= llama_model.sample(\n",
    "                input_ids,\n",
    "                logits_processor=logits_processor,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "\n",
    "        # clean up output\n",
    "        sequences = outputs.tolist()\n",
    "        output_ids = []\n",
    "        sequence_ids = []\n",
    "        logits_mask = []\n",
    "        for seq in sequences:                    \n",
    "            seq = seq[len(prompt_tokens):]                    \n",
    "            while seq[-1] == 0:\n",
    "                seq = seq[:-1]\n",
    "            while seq[-1] == 2:\n",
    "                seq = seq[:-1]\n",
    "            # for i in range(min(len(suffix_tokens), len(seq)), 0, -1):\n",
    "            #     if seq[-i:] == list(suffix_tokens[:i]):\n",
    "            #         seq = seq[:-i]\n",
    "            #         break\n",
    "            output_ids.append(seq)\n",
    "            sequence_ids.append(list(prompt_tokens) + list(seq) + list(suffix_tokens))\n",
    "            # after_prefix = len(seq) + len(suffix_tokens)\n",
    "            # check_logits_len = min(after_prefix, len(seq) * 2 + 10)\n",
    "            # logits_mask.append([0.0] * len(prompt_tokens) + [1.0] * check_logits_len + [0.0] * (after_prefix - check_logits_len))\n",
    "            check_logits_len = min(1, len(suffix_tokens))\n",
    "            # check_logits_len = len(suffix_tokens)\n",
    "            logits_mask.append([0.0] * (len(prompt_tokens) + len(seq)) + [1.0] * check_logits_len + [0.0] * (len(suffix_tokens)-check_logits_len))\n",
    "        max_len = max([len(x) for x in sequence_ids])\n",
    "        sequence_ids = [x + [0] * (max_len - len(x)) for x in sequence_ids]\n",
    "        # print(sequence_ids)\n",
    "        logits_mask = [x + [0.0] * (max_len - len(x)) for x in logits_mask]\n",
    "        sequence_ids = torch.tensor(sequence_ids, device=device)\n",
    "        logits_mask = torch.tensor(logits_mask, device=device)\n",
    "\n",
    "        # Get returns\n",
    "        sequences_scores = get_sequence_scores(llama_model, sequence_ids, logits_mask)\n",
    "        outputs_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        if args.debug:\n",
    "            top_k = 64\n",
    "            print(f\"-------------------- Top-${top_k} -----------------------\")\n",
    "            sequence_rank = torch.argsort(sequences_scores, descending=True).tolist()\n",
    "            for sequence_idx in sequence_rank[:top_k]:\n",
    "                print(f\"#{sequence_idx}: {sequences_scores[sequence_idx]}\", outputs_texts[sequence_idx])\n",
    "                print(output_ids[sequence_idx])\n",
    "        \n",
    "        # This will return it as text. You can further use json.loads(r.text) to retrieve the results\n",
    "        return {\n",
    "            \"beam_outputs_texts\": outputs_texts,\n",
    "            \"beam_outputs_sequences_scores\": sequences_scores,\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Word Length\n",
      "max_length: 78, max_tokens: 50\n",
      "Logits Processor:  [<model_utils.ConstraintLogitsProcessor object at 0x7f1218210f50>]\n",
      "-------------------- Top-$64 -----------------------\n",
      "#14: -0.037928611040115356 Once upon a time in the state of California.\n",
      "[9038, 2501, 263, 931, 297, 278, 2106, 310, 8046, 29889]\n",
      "#13: -0.06619183719158173 She was alone, distraught and hopeless.\n",
      "[2296, 471, 7432, 29892, 1320, 336, 688, 400, 322, 8171, 6393, 29889]\n",
      "#11: -0.06985049694776535 Long ago, in a land far, far away...**In the time of old** *******In most mystical boundless lands.******* ****Within this tale now.\n",
      "[6242, 8020, 29892, 297, 263, 2982, 2215, 29892, 2215, 3448, 856, 1068, 797, 278, 931, 310, 2030, 1068, 334, 2328, 1068, 797, 1556, 16624, 936, 3216, 2222, 12625, 29889, 2328, 17435, 334, 17435, 3047, 262, 445, 17694, 1286, 29889]\n",
      "#12: -0.07931356877088547 Once upon a time, a Bad Fairy named Draca did indeed live in a land that was known as Storyville.\n",
      "[9038, 2501, 263, 931, 29892, 263, 9178, 13822, 29891, 4257, 360, 945, 29874, 1258, 6200, 5735, 297, 263, 2982, 393, 471, 2998, 408, 13740, 4909, 29889]\n",
      "#10: -0.08249200880527496 In a land not so far away, in a land of enchantment and adventure? And in a castle on a hill?\n",
      "[512, 263, 2982, 451, 577, 2215, 3448, 29892, 297, 263, 2982, 310, 427, 13775, 358, 322, 17623, 545, 29973, 1126, 297, 263, 20610, 373, 263, 17306, 29973]\n",
      "#15: -0.08458206802606583 Once upon a time, in a field by the edge of a forest...There was a witch.\n",
      "[9038, 2501, 263, 931, 29892, 297, 263, 1746, 491, 278, 7636, 310, 263, 13569, 856, 8439, 471, 263, 281, 2335, 29889]\n",
      "#4: -0.08891180902719498 a princess lived in a castle high on a hill. She was beautiful, kind, and very elegant. All the princes in the kingdom wanted?\n",
      "[263, 12456, 985, 10600, 297, 263, 20610, 1880, 373, 263, 17306, 29889, 2296, 471, 9560, 29892, 2924, 29892, 322, 1407, 19232, 29889, 2178, 278, 23251, 297, 278, 20748, 5131, 29973]\n",
      "#6: -0.09070175886154175 There was a great black snake. His name was Slither. Slither lived in the big city, in a big tower near the river.\n",
      "[1670, 471, 263, 2107, 4628, 269, 21040, 29889, 3600, 1024, 471, 14866, 2121, 29889, 14866, 2121, 10600, 297, 278, 4802, 4272, 29892, 297, 263, 4802, 19372, 2978, 278, 8580, 29889]\n",
      "#5: -0.09445687383413315 There were three mice named Frederick, George and Wilhelmina. They lived together in a great, big, beautiful maze. Their home was in one section.\n",
      "[1670, 892, 2211, 286, 625, 4257, 19769, 29892, 5122, 322, 10756, 1099, 29889, 2688, 10600, 4208, 297, 263, 2107, 29892, 4802, 29892, 9560, 611, 911, 29889, 11275, 3271, 471, 297, 697, 4004, 29889]\n",
      "#9: -0.09485507011413574 in a land far, far away there lived a princess named Sapphire. She was the prettiest and most kind-hearted princess there ever was.\n",
      "[297, 263, 2982, 2215, 29892, 2215, 3448, 727, 10600, 263, 12456, 985, 4257, 317, 932, 14812, 29889, 2296, 471, 278, 758, 698, 12239, 322, 1556, 2924, 29899, 23057, 287, 12456, 985, 727, 3926, 471, 29889]\n",
      "#3: -0.09574922174215317 Once upon a time, a big orange tabby cat named Simba decided to stray from the safety of his own home. Simba lived alone.\n",
      "[9038, 2501, 263, 931, 29892, 263, 4802, 24841, 4434, 1609, 6635, 4257, 3439, 2291, 8459, 304, 380, 764, 515, 278, 15332, 310, 670, 1914, 3271, 29889, 3439, 2291, 10600, 7432, 29889]\n",
      "#8: -0.1005990132689476 In a land far, far away, there was a kingdom ruled by a fearless and just king. This kingdom, known as Abosia, was renownedforitspecialabilities.\n",
      "[512, 263, 2982, 2215, 29892, 2215, 3448, 29892, 727, 471, 263, 20748, 29490, 491, 263, 8866, 2222, 322, 925, 6989, 29889, 910, 20748, 29892, 2998, 408, 1976, 359, 423, 29892, 471, 4325, 26689, 1454, 1169, 412, 1455, 370, 9770, 29889]\n",
      "#0: -0.10278414189815521 Once upon a time, when magic was all around, there lived a brave and gallant knight named Sir Reginald. Reggie, as his friends called?him.\n",
      "[9038, 2501, 263, 931, 29892, 746, 15709, 471, 599, 2820, 29892, 727, 10600, 263, 26565, 322, 11798, 424, 889, 523, 4257, 6290, 28124, 2741, 29889, 2169, 12053, 29892, 408, 670, 7875, 2000, 29973, 26994, 29889]\n",
      "#7: -0.10542847216129303 Once upon an era where time was fluid and events unfolded with each passing moment. In this era, a once prosperous nation found itself\"\n",
      "[9038, 2501, 385, 3152, 988, 931, 471, 22576, 322, 4959, 20220, 287, 411, 1269, 6819, 3256, 29889, 512, 445, 3152, 29892, 263, 2748, 25831, 681, 5233, 1476, 3528, 29908]\n",
      "#2: -0.12854263186454773 Once upon a time in a faraway kingdom, there was a young girl named Lily.\n",
      "[9038, 2501, 263, 931, 297, 263, 2215, 21694, 20748, 29892, 727, 471, 263, 4123, 7826, 4257, 365, 2354, 29889]\n",
      "#1: -0.174859419465065 once upon a time, there was a prince charming. He lived in a picturesque village, surrounded by rolling hills and sparkling lakes. The prince?s?carefree?days?were?spent?riding?\n",
      "[2748, 2501, 263, 931, 29892, 727, 471, 263, 15927, 1373, 4056, 29889, 940, 10600, 297, 263, 14956, 802, 5720, 29892, 22047, 491, 27777, 22696, 322, 16267, 1847, 425, 10794, 29889, 450, 15927, 29973, 29879, 29973, 18020, 9021, 29973, 16700, 29973, 29893, 406, 29973, 1028, 296, 29973, 2429, 292, 29973]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "# input_json = {\n",
    "#     'Prefix': 'Once upon a time there was an old mother pig who had one hundred little pigs and not enough food to feed them. So when they were old enough, ', \n",
    "#     'Suffix': 'a story about the 92nd little pig.', \n",
    "#     'Instruct': '', \n",
    "#     'Prior': '', \n",
    "#     'Operation': 'Continuation', \n",
    "#     'max_tokens': 50, \n",
    "#     'temperature': 0.95, \n",
    "#     'num_return_sequences': 10, \n",
    "#     'num_beams': 10, \n",
    "#     'no_repeat_ngram_size': 2, \n",
    "#     'top_p': 1.0\n",
    "# }\n",
    "\n",
    "input_json = {\n",
    "    'temperature': 0.8,\n",
    "    'num_return_sequences': 5, \n",
    "    'num_beams': 64, \n",
    "    'no_repeat_ngram_size': -1, \n",
    "    'top_p': 1.0,\n",
    "    'token_constraint': [1, 64], \n",
    "}\n",
    "\n",
    "example_0 = { # ERROR\n",
    "    'Prefix': 'Once upon a time there was an old mother pig who had one hundred little pigs and not enough food to feed them. So when they were old enough, ', \n",
    "    'Suffix': 'a story about the 92nd little pig.', \n",
    "    'Instruct': '', \n",
    "    'Prior': '', \n",
    "    'Operation': 'Insertion', \n",
    "    'word_contraint': [10, 20],\n",
    "    'keyword_constraint': [],\n",
    "}\n",
    "\n",
    "example_0a = { # ERROR\n",
    "    'Prefix': 'Once upon a time there was an old mother pig who had one hundred little pigs and not enough food to feed them. So when they were old enough, ', \n",
    "    'Suffix': 'a story about the 92nd little pig.', \n",
    "    'Instruct': '', \n",
    "    'Prior': '', \n",
    "    'Operation': 'Insertion', \n",
    "    'word_contraint': [],\n",
    "    'keyword_constraint': [],\n",
    "}\n",
    "\n",
    "example_1 = {\n",
    "    'Prefix': \"When you die, you appear in a cinema with a number of other people who look like you. You find out that they are your previous reincarnations, and soon you all begin watching your next life on the big screen.\\n\\nYou are seated at the head of a table in a crowded conference room.\",\n",
    "    # 'Suffix': \" He is a film director, and he has a movie he wants to make. He says he wants to make a movie of your next life.\\n\\n\\\"Just wait a minute,\\\" you say, \\\"I am watching your movie right now.\\\"\\n\\n\\\"That's because must have agreed to make it and you are watching the result,\\\" he says.\\n\\\"I don't remember agreeing to make a movie of my next life,\\\" you say.\\n \\\"Of coyrse not,\\\" replied the man, \\\" that is not how these things work.\\\"\\n\\\"How do they work?\\\" you ask.\\n\\n\\n\\\"Youe brain is erased between lives,\\\" says the man. \\\"Therefore, you don't remember making the movie.\\\"\\n\\n\\\"But this is supposed to be my future, not my past,\\\" you say.\\n\\\"It is your future,\\\" says the man, \\\"but it is also your past.\\n\\\"\\n\\n\\\"How can that be?,\\\" you reply.\\n\\\"It is a paradox,\\\" says the man.\\n\\n\\n\\\"Paradox? You means its a scmm, don't you?\\\" you reply.\\n\\\"No, no,\\\" says the man, \\\"it is not a scam.\\n Well, maybe its a scam, I forget.\\\"\\n\\\"You mean you don't know whether it is a scam or not?\\\" you ask.\\n\\n\\n\\\"I mean I don't remember making this movie at all,\\\" says the man.\\n\\\"But you are the director,\\\" you say.\\n\\n\\\"I am the director,\\\" says the man, \\\"but I don't remember making this movie.\\n\\\"\\n\\n\\\"Are you an idiot?\\\" you ask the man.\\n\\\"Yes,\\\" says the man, \\\"I am an idiot.\\\" But I'm also you in your next life.\\n\\n\\\"That's impossible,\\\" you say.\\n\\n\\n\\\"If it were impossible, could I do this?\\\" asks the man, and he steps out of the screen and takes a seat next to you.\\n\\\"But I'm not dead,\\\" you say.\\n\\n\\n\\\" You, me... we all are,\\\" says the man. \\\"Always will be too.\\\"\\n\\\"I don't understand,\\\" you say.\\n\\n\\nAt that, the man walks right up to you and stands chett to chest. Suddenly you are the man, and he is you.\\n\\\"Do you understand now?\\\" you ask.\\n\\n\\n\\\"Yes,\\\" you answer, and take your seat to watch the rest of the movie.\",\n",
    "    'Suffix': \" a film director, and he has a movie he wants to make. He says he wants to make a movie of your next life.\\n\\n\\\"Just wait a minute,\\\" you say, \\\"I am watching your movie right now.\",\n",
    "    'Instruct': ' and talk about teanage mutant ninja turtles', \n",
    "    # 'Instruct': '', \n",
    "    'Prior': '', \n",
    "    'Operation': 'Continuation', \n",
    "    'word_contraint': [],\n",
    "    'keyword_constraint': [],\n",
    "}\n",
    "\n",
    "example_2 = {\n",
    "    'Prefix': 'An alien has kidnapped Matt Damon, not knowing what lengths humanity goes through to retrieve him whenever he goes missing.',\n",
    "    # 'Suffix': \" They reveal that the reason aliens cannot speak English is because their mouths are not capable of pronouncing vowels, and as a result, they employ humans to speak for them.\\n\\nMatt Damon is shocked by all of this and obviously doesn't want to be involved with aliens. He tells that aliens that he wants to be traded back for the alien prince so he can return to the United States.\\n\\nThe aliens explain that there is a possibility of that but it's going to cost the United States a big time price. The aliens work out a deal and tell Matt Damon that they want to live in the United States. They don't want to live with humans and instead want to live in their own community.\\n\\nMatt Damon has no idea how he can make that happen but again, Damon is loved by the United States and all of it's people. He calls the President of the United States and the President tells him there is no possible place for the aliens to go. The country has 50 states and humans inhabit all of those states.\\n\\nDamon tells the aliens the bad news and they explain to him that he's stuck with them. The interesting thing is that as Damon continued to live with the aliens, the more he started to enjoy it.\\n\\nThe alien world didn't treat him like a famous actor. He didn't have to deal with the constant stress and scrutiny of being a Hollywood star. The aliens treated him well. He explained to the aliens how great their planet was. But he was very upset about one thing. He wanted to see his family. \\n\\nHe asked the aliens if there was any possibility that he could go back to the United States so he could see his family. They told him that he couldn't but they could abduct his family and bring them to their home planet.\\n\\nDamon thought long and hard about that and accepted the offer. The aliens ended up abducting Matt Damon's family and brought them to him. They were extremely happy to see him but also wanted to go home. The issue is that they were stuck there like Damon as well.\\n\\nBut just like Matt, the more time they spent on the aliens planet, the more they enjoyed it. They were treated extremely well and had fun learning about the alien's culture and all of the technology they had access to. They're stuck on that planet to this day, but they enjoy living there much more than they thought they would.\",\n",
    "    'Suffix': \" They reveal that the reason aliens cannot speak English is because their mouths are not capable of pronouncing vowels, and as a result, they employ humans to speak for them.\\n\\n\",\n",
    "    'Instruct': '', \n",
    "    'Prior': '', \n",
    "    'Operation': 'Insertion', \n",
    "    'word_contraint': [],\n",
    "    'keyword_constraint': [\n",
    "        \"agree\",\n",
    "        \"prince\"],\n",
    "}\n",
    "\n",
    "test_example = {'Prefix': 'Once upon a time ...? ', 'Suffix': '', 'Prior': '', 'Instruct': '', 'word_contraint': (20, 25), 'keyword_constraint': [], 'temperature': 0.95, 'num_return_sequences': 5, 'num_beams': 16, 'no_repeat_ngram_size': -1, 'top_p': 1.0, 'token_constraint': [1, 50]}\n",
    "test_example['Operation'] = \"Continuation\"\n",
    "\n",
    "input_json.update(test_example)\n",
    "\n",
    "args.do_beam_search = False\n",
    "args.debug = True\n",
    "prompt(input_json)\n",
    "print(\"-------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you die, you appear in a cinema with a number of other people who look like you. You find out that they are your previous reincarnations, and soon you all begin watching your next life on the big screen.\n",
      "\n",
      "You are seated at the head of a table in a crowded conference room.\n",
      "-----------\n",
      " a story about the 92nd little pig.</s>\n",
      "\n",
      " They\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode((1932, 366, 762, 29892, 366, 2615, 297, 263, 24615, 411, 263, 1353, 310, 916, 2305, 1058, 1106, 763, 366, 29889, 887, 1284, 714, 393, 896, 526, 596, 3517, 337, 3742, 2753, 800, 29892, 322, 4720, 366, 599, 3380, 21217, 596, 2446, 2834, 373, 278, 4802, 4315, 29889, 13, 13, 3492, 526, 409, 630, 472, 278, 2343, 310, 263, 1591, 297, 263, 11660, 7176, 21362, 5716, 29889)))\n",
    "print(\"-----------\")\n",
    "print(tokenizer.decode((263, 5828, 1048, 278, 29871, 29929, 29906, 299, 2217, 282, 335, 29889, 2)))\n",
    "print(tokenizer.decode((13, 2688)))\n",
    "tokenizer.encode(\"\\n They\")\n",
    "prior = \" A man is speaking to you.\"\n",
    "print(len(tokenizer.encode(prior)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------- Top-3 -----------------------\n",
    "#5:  she sent them out into the world to find some food and bring back\n",
    "#9:  she sent them out into the world to find food for themselves. She told them to be careful and find\n",
    "#1:  she sent the little pigs out into the world to try and find food, and bring back\n",
    "\n",
    "-------------------- Top-3 -----------------------\n",
    "#7:  she sent them out into the world to find jobs. The first one came back with\n",
    "#8:  she sent each of her little pigs out to find\n",
    "#9:  she sent her little pigs out into the world in search of\n",
    "\n",
    "# Example 0a\n",
    "-------------------- Top-3 -----------------------\n",
    "#5:  she sent her little pigs to the market to try to sell them. This is\n",
    "#9:  she sent them out into the world to find food and a home. One hundred little pigs went out into the world and only one came back with\n",
    "#14:  she sent the little pigs out into the world to find their fortune. She gave each little pig\n",
    "\n",
    "-------------------- Top-3 -----------------------\n",
    "#1:  he sent them out into the world to find work and bring home\n",
    "#8:  she sent them out into the world to find their fortunes and bring back\n",
    "#12:  the mother sent all but 92 of them into the forest to find\n",
    "\n",
    "# Example 1\n",
    "-------------------- Top-3 -----------------------\n",
    "#7:  A man is giving a presentation to a room full of people.\n",
    "#13:  The man sitting next to you is your boss.\n",
    "#3:  You watch as your next reincarnation approaches the table and sits across from you.\n",
    "\n",
    "-------------------- Top-3 ----------------------- (Trunc Suffix)\n",
    "#2:  You look at the man sitting at the other end of the table.\n",
    "#4:  There is a man sitting to your right, smiling at you.\n",
    "#15:  You are talking to a man named Jack.\n",
    "\n",
    "-------------------- Top-3 -----------------------\n",
    "#15:  The alien prince, obsession with Earth and humans, has decided to agree to the human demand.\n",
    "#4:  The alien prince, seemingly oblivious to the obsession humans have with Matt Damon, agree to release him unharmed.\n",
    "#11:  A rescue is initiated to save Matt Damon from the clutches of an otherworldly obsession, to which no prince would agree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
