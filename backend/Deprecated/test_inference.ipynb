{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local1/ponienkung/miniconda3/envs/coreset/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-05 12:43:44,340] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name_or_path = \"../output/Story-TULU-LLAMA2-7B-Cont/\"\n",
    "inference_file = \"../data/processed/writing_prompts/wp_allcont_debug.jsonl\"\n",
    "# Load Model and Tokenizer\n",
    "# Add pdding left to be able to do batched generation\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def encode_with_messages_format(example, tokenizer, max_seq_length):\n",
    "    '''\n",
    "    Here we assume each example has a 'messages' field Each message is a dict with 'role' and 'content' fields.\n",
    "    We concatenate all messages with the roles as delimiters and tokenize them together.\n",
    "    '''\n",
    "    messages = example['messages']\n",
    "    if len(messages) == 0:\n",
    "        raise ValueError('messages field is empty.')\n",
    "    \n",
    "    def _concat_messages(messages):\n",
    "        message_text = \"\"\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"system\":\n",
    "                message_text += \"<|system|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"user\":\n",
    "                message_text += \"<|user|>\\n\" + message[\"content\"].strip() + \"\\n\"\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                # For assistant, we don't want to give the content for inference\n",
    "                message_text += \"<|assistant|>\\n\" # + message[\"content\"].strip() + tokenizer.eos_token + \"\\n\"\n",
    "                # Add a break to make sure there's only one turn\n",
    "                break\n",
    "            else:\n",
    "                raise ValueError(\"Invalid role: {}\".format(message[\"role\"]))\n",
    "        return message_text\n",
    "        \n",
    "    example_text = _concat_messages(messages).strip()\n",
    "    tokenized_example = tokenizer(example_text, return_tensors='pt', max_length=max_seq_length, truncation=True)\n",
    "    input_ids = tokenized_example.input_ids\n",
    "    # Do not get labels here, no need to\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    return {\n",
    "        'input_ids': input_ids.flatten(),\n",
    "        'attention_mask': attention_mask.flatten(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'writing_prompts', 'id': 'writing_prompts_0', 'messages': [{'role': 'user', 'content': \"Continue the given text in enchanting style:\\nI watched as the seconds chipped the last of my business hours away. The expected tinkle signalling the entrance of my oddest regular interrupted the deafening ticks of the little hand I had been tracking. So familiar was our weekly routine that I didn't even notice I had begun until the tip of my tattoo gun dipped into John's skin to cross out the last of the names on his back. I didn't need to read it to know Gregory was being offed. For what, I never asked. A few months ago, John had sauntered into my humble tattoo studio right at closing time. Not a second before. I had attempted to turn John away, especially having just refused the business of three, piss-drunk dude-bros who didn't want to hear that matching knuckle-pieces spelling out B.R.O.S. on their right hands was not the best, most awesome idea ever. Instead of arguing or leaving, John had stood stock-still. Silent. Staring straight into my heavy-lidded eyes. After another weak attempt at explaining I was closing shop, I had sighed and gone through my usual first-time client routine. The first had\\n\\n\"}, {'role': 'assistant', 'content': \"been a shocking discovery of how much John trusted me with his battered body. The second had been a silent understanding that we would never discuss the why of his tattoos, or why he chose to etch names of people who seemed to mean something to him, only to cross them out later. And now, here we were, in this moment, his back adorned with the remnants of his past; a tapestry of inked memories that told a story only he and I could decipher.\\n\\nAs the final stroke of my needle closed off Gregory's name, a surge of anticipation filled the air. I sensed it, John sensed it. It was as if the old spirits that lingered in the shadows of my studio were whispering secrets that only he could hear. The power of those whispered words seemed to dance across John's skin, electrifying the room, and sending shivers down my spine. It was in these moments, when the barriers of everyday life were stripped away, that I felt the true magic in my craft.\\n\\nWith a flick of my wrist, I wiped away the excess ink and took a step back to admire my work. The words that were once a part of John's narrative were now hidden beneath the inky veil, locked away from prying eyes, and tucked deep within the recesses of his being. And yet, they were not forgotten. For whenever John looked in the mirror, he would see the crossed-out names, a reminder of the chapters he had closed, the pain he had endured, and the strength he had found to move forward.\\n\\nBut as I gazed into John's eyes, I saw a weariness that went beyond the mark of the needle. There was a heaviness that weighed upon him, an invisible burden that seemed to shadow his every move. I knew then that it was not just the names he wanted to erase. It was the memories, the emotions, and the scars that clung to his soul. And as his trust in me grew, so did my commitment to him, to use my artistry not just to decorate the surface, but to heal the wounds beneath.\\n\\nTaking a deep breath, I gently laid my hand on John's trembling shoulder, my touch a reassurance that he was not alone. The warmth of my palm seeped into his skin, mingling with the ink, and for a brief moment, our energies intertwined. In that ethereal connection, I whispered the ancient words that had been passed down through generations of tattoo artists, invoking the spirits of creativity and transformation.\\n\\nAs the incantation danced upon the air, a soft glow enveloped the room, casting a mystical aura upon us. The ticking of the clock ceased, and time became irrelevant in this enchanted space. I closed my eyes, surrendering myself to the magic that flowed through me, opening myself to the whispers of the spirits that guided me. And in that ethereal reverie, I vowed to unlock the secrets buried within John's heart, offering him not just a physical transformation but an emotional one as well.\\n\\nWith steady hands and a heart full of compassion, I reached for my tattoo gun once more. This time, the ink that flowed from its tip held within it the power to mend, to soothe, and to bring forth a new beginning. Together, John and I embarked on a journey of healing, as the strokes of my needle traced delicate patterns across his skin, invoking a tapestry of hope, resilience, and self-discovery. And in each new tattoo, a part of John's past faded away, making room for the birth of a brighter future.\\n\\nAs the last remnants of ink settled upon John's refreshed canvas, a hushed silence filled the studio. The spirits, content with their work, retreated into the hidden realms from where they had emerged. I slowly removed my hand from John's shoulder, feeling a profound sense of awe and satisfaction. The weight that had burdened him seemed to have dissipated, replaced by a newfound lightness in his eyes and a smile that stretched from ear to ear.\\n\\nWith a tender embrace, John thanked me for more than just the artistry I had bestowed upon him. He thanked me for being a companion on his journey, for standing beside him as he faced his demons, and for helping him redefine his story. And in that moment, as we bid farewell, I knew that John's transformative experience would forever be etched not just on his skin but in the depths of my own soul.\\n\\nFor within the enchanted world of my tattoo studio, where the ordinary merged with the extraordinary, where the ink flowed like rivers of magic, I had witnessed the power of art to heal, to uplift, and to embrace the wounds that make us who we are. And as I watched John disappear into the realm of possibilities, I knew that my humble studio would forever remain a haven, a sanctuary for those seeking solace, transformation, and the enchantment that lies hidden within the marks of our lives.\"}]}\n",
      "<s> <|user|>\n",
      "Continue the given text in enchanting style:\n",
      "I watched as the seconds chipped the last of my business hours away. The expected tinkle signalling the entrance of my oddest regular interrupted the deafening ticks of the little hand I had been tracking. So familiar was our weekly routine that I didn't even notice I had begun until the tip of my tattoo gun dipped into John's skin to cross out the last of the names on his back. I didn't need to read it to know Gregory was being offed. For what, I never asked. A few months ago, John had sauntered into my humble tattoo studio right at closing time. Not a second before. I had attempted to turn John away, especially having just refused the business of three, piss-drunk dude-bros who didn't want to hear that matching knuckle-pieces spelling out B.R.O.S. on their right hands was not the best, most awesome idea ever. Instead of arguing or leaving, John had stood stock-still. Silent. Staring straight into my heavy-lidded eyes. After another weak attempt at explaining I was closing shop, I had sighed and gone through my usual first-time client routine. The first had\n",
      "<|assistant|>taken me by surprise, the second had been a bit of a challenge, but John... John had been different from the start. His presence was ethereal, like a whisper of wind through ancient trees. His eyes, a mesmerizing shade of emerald, held a depth that seemed to hold secrets\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><s> <|user|>\n",
      "Continue the given text in descriptive style:\n",
      "*Officer walks into the tent*\"Sir we have a letter from the Americans\"*Here let me have a look*> You frickin fricks can't EVER be quenched. MY ARMY CAN'T EVER BE QUENCHED. YOU FRICKIN FRICKS, WHEN WILL YOU LEARN. **WHEN WILL YOU LEARN**> YOU\n",
      "<|assistant|>\n",
      "frickin fricks can't EVER be quenched. MY ARMY CAN'T EVER BE QUENCHED. YOU FRICKIN FRICKS, WHEN WILL YOU LEARN. **WHEN WILL YOU LEARN**> YOU frick\n",
      "---------------Done\n",
      "['taken me by surprise, the second had been a bit of a challenge, but John... John had been different from the start. His presence was ethereal, like a whisper of wind through ancient trees. His eyes, a mesmerizing shade of emerald, held a depth that seemed to hold secrets', \"\\nfrickin fricks can't EVER be quenched. MY ARMY CAN'T EVER BE QUENCHED. YOU FRICKIN FRICKS, WHEN WILL YOU LEARN. **WHEN WILL YOU LEARN**> YOU frick\"]\n",
      "taken me by surprise, the second had been a bit of a challenge, but John... John had been different from the start. His presence was ethereal, like a whisper of wind through ancient trees. His eyes, a mesmerizing shade of emerald, held a depth that seemed to hold secrets\n",
      "frickin fricks can't EVER be quenched. MY ARMY CAN'T EVER BE QUENCHED. YOU FRICKIN FRICKS, WHEN WILL YOU LEARN. **WHEN WILL YOU LEARN**> YOU frick\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side = \"left\")\n",
    "inference_batch_size = 2\n",
    "\n",
    "# Load the dataset from inference file\n",
    "dataset_args = {}\n",
    "raw_inference_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"inference\": inference_file},\n",
    "    **dataset_args,\n",
    ")\n",
    "print(raw_inference_dataset['inference'][0])\n",
    "# Preprocessing the datasets.\n",
    "encode_function = partial(\n",
    "    encode_with_messages_format,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "inference_dataset = raw_inference_dataset.map(\n",
    "    encode_function,\n",
    "    batched=False,\n",
    "    num_proc=8,\n",
    "    remove_columns=[name for name in raw_inference_dataset[\"inference\"].column_names if name not in [\"input_ids\", \"labels\", \"attention_mask\"]],\n",
    "    desc=\"Tokenizing and reformatting instruction data\",\n",
    ")\n",
    "inference_dataset.set_format(type=\"pt\")\n",
    "inference_dataset = inference_dataset[\"inference\"]\n",
    "# inference_dataset = inference_dataset.filter(lambda example: (example['labels'] != -100).any())[\"inference\"]\n",
    "inference_dataloader = DataLoader(\n",
    "    inference_dataset, \n",
    "    shuffle=False, \n",
    "    collate_fn=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=\"longest\"),\n",
    "    batch_size=inference_batch_size\n",
    ")\n",
    "# # Load Testing Dataset\n",
    "\n",
    "model.half().cuda()\n",
    "\n",
    "# Start doing inference\n",
    "for batch in inference_dataloader:\n",
    "    generation_output = model.generate(\n",
    "        batch.input_ids.cuda(),\n",
    "        max_new_tokens=65, \n",
    "        # return_dict_in_generate=True,\n",
    "        # output_scores=True,\n",
    "        do_sample=True, # If want to use greedy, set to False\n",
    "        attention_mask = batch.attention_mask.cuda()\n",
    "    )\n",
    "    # You can also get the scores(logits) by changing above arfs\n",
    "    output_ids = generation_output\n",
    "    # All input have same length with paddings\n",
    "    input_len = batch.input_ids[0].shape[-1]\n",
    "    output_str_list = tokenizer.batch_decode(output_ids[:,input_len:])\n",
    "    print(tokenizer.batch_decode(output_ids)[0])\n",
    "    print(tokenizer.batch_decode(output_ids)[1])\n",
    "    print(\"---------------Done\")\n",
    "    print(output_str_list)\n",
    "    for output_str in output_str_list:\n",
    "        print(output_str.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|user|>\n",
      "Continue the given text in enchanting style:\n",
      "I watched as the seconds chipped the last of my business hours away. The expected tinkle signalling the entrance of my oddest regular interrupted the deafening ticks of the little hand I had been tracking. So familiar was our weekly routine that I didn't even notice I had begun until the tip of my tattoo gun dipped into John's skin to cross out the last of the names on his back. I didn't need to read it to know Gregory was being offed. For what, I never asked. A few months ago, John had sauntered into my humble tattoo studio right at closing time. Not a second before. I had attempted to turn John away, especially having just refused the business of three, piss-drunk dude-bros who didn't want to hear that matching knuckle-pieces spelling out B.R.O.S. on their right hands was not the best, most awesome idea ever. Instead of arguing or leaving, John had stood stock-still. Silent. Staring straight into my heavy-lidded eyes. After another weak attempt at explaining I was closing shop, I had sighed and gone through my usual first-time client routine. The first had\n",
      "<|assistant|>taken me by surprise, the second had been a bit of a challenge, but John... John had been different from the start. His presence was ethereal, like a whisper of wind through ancient trees. His eyes, a mesmerizing shade of emerald, held a depth that seemed to hold secrets\n",
      "<|user|>\n",
      "Continue the given text in descriptive style:\n",
      "*Officer walks into the tent*\"Sir we have a letter from the Americans\"*Here let me have a look*> You frickin fricks can't EVER be quenched. MY ARMY CAN'T EVER BE QUENCHED. YOU FRICKIN FRICKS, WHEN WILL YOU LEARN. **WHEN WILL YOU LEARN**> YOU\n",
      "<|assistant|>\n",
      "frickin fricks can't EVER be quenched. MY ARMY CAN'T EVER BE QUENCHED. YOU FRICKIN FRICKS, WHEN WILL YOU LEARN. **WHEN WILL YOU LEARN**> YOU frick\n",
      "---------------Done\n",
      "['taken me by surprise, the second had been a bit of a challenge, but John... John had been different from the start. His presence was ethereal, like a whisper of wind through ancient trees. His eyes, a mesmerizing shade of emerald, held a depth that seemed to hold secrets', \"\\nfrickin fricks can't EVER be quenched. MY ARMY CAN'T EVER BE QUENCHED. YOU FRICKIN FRICKS, WHEN WILL YOU LEARN. **WHEN WILL YOU LEARN**> YOU frick\"]\n",
      "taken me by surprise, the second had been a bit of a challenge, but John... John had been different from the start. His presence was ethereal, like a whisper of wind through ancient trees. His eyes, a mesmerizing shade of emerald, held a depth that seemed to hold secrets\n",
      "frickin fricks can't EVER be quenched. MY ARMY CAN'T EVER BE QUENCHED. YOU FRICKIN FRICKS, WHEN WILL YOU LEARN. **WHEN WILL YOU LEARN**> YOU frick\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "generation_output = model.generate(\n",
    "    batch.input_ids.cuda(),\n",
    "    max_new_tokens=65, \n",
    "    # return_dict_in_generate=True,\n",
    "    # output_scores=True,\n",
    "    do_sample=False,\n",
    "    attention_mask = batch.attention_mask.cuda()\n",
    ")\n",
    "# You can also get the scores(logits) by changing above arfs\n",
    "output_ids = generation_output\n",
    "# All input have same length with paddings\n",
    "input_len = batch.input_ids[0].shape[-1]\n",
    "output_str_list = tokenizer.batch_decode(output_ids[:,input_len:])\n",
    "print(tokenizer.batch_decode(output_ids)[0])\n",
    "print(tokenizer.batch_decode(output_ids, skip_special_tokens = True)[1])\n",
    "print(\"---------------Done\")\n",
    "print(output_str_list)\n",
    "for output_str in output_str_list:\n",
    "    print(output_str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----t was in that moment, as I gazed into her eyes, that I made my decision. The air around us seemed to thicken, the atmosphere charged with an electric tension. I could feel the weight of the words she had spoken, the implication of her warning. It was a challenge, a dare-------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name_or_path = \"../output/Story-TULU-LLAMA2-7B-Cont/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side = \"left\")\n",
    "\n",
    "print(f\"----{tokenizer.decode([29873, 471, 297, 393, 3256, 29892, 408, 306, 12642, 287, 964, 902, 5076, 29892, 393, 306, 1754, 590, 10608, 29889, 450, 4799, 2820, 502, 6140, 304, 12003, 264, 29892, 278, 25005, 20139, 411, 385, 12646, 260, 2673, 29889, 306, 1033, 4459, 278, 7688, 310, 278, 3838, 1183, 750, 19182, 29892, 278, 2411, 1414, 310, 902, 9177, 29889, 739, 471, 263, 18766, 29892, 263, 23222])}-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "prefix = \"I really love my dog, who\"\n",
    "model.half().cuda()\n",
    "# Test beam generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import (\n",
    "    parse_prompt, parse_suggestion, parse_probability,\n",
    "    filter_suggestions\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    input_ids = encode_with_messages_format(prefix, tokenizer).cuda()\n",
    "    # dict_keys(['sequences', 'sequences_scores', 'scores', 'beam_indices', 'attentions', 'hidden_states'])\n",
    "    beam_outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.95,\n",
    "        num_return_sequences=5,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_p=1,\n",
    "        early_stopping=True,\n",
    "        output_scores = True, # Get sequences_scores\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "suggestions = []\n",
    "for choice in beam_outputs.sequences:\n",
    "    choice_text = tokenizer.batch_decode(choice[input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    suggestion = parse_suggestion(\n",
    "        choice_text,\n",
    "        results['after_prompt'],\n",
    "        stop_rules\n",
    "    )\n",
    "    probability = parse_probability(beam_outputs.sequences_scores.cpu().detach().tolist())\n",
    "    suggestions.append((suggestion, probability, engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sequences', 'sequences_scores', 'scores', 'beam_indices', 'attentions', 'hidden_states'])\n",
      "[\"is a rescue dog. I have had him since he was a puppy and he has brought so much joy to my life. He is my baby, my best friend, and my confidant. We do everything together and I couldn't imagine life without him.\", \"is my best friend. I would do anything for him. He's always there for me, no matter what. When I'm sad, he lays his head on my lap and looks up at me with those big, loving eyes, and I know everything is going to be okay.\\n\\nBut sometimes, I can't help but wonder if he feels the same way about me. Does he know how much I love him? Do I ever cross his mind when he'\", 'is a rescue dog. I have had him since he was 8 weeks old. He is now 12 years old and still going strong. We have been through a lot together and I couldn\\'t imagine my life without him.\\n\\nHe has been my best friend, my confidant, and my protector. When I am sad, he lays his head on my lap and looks at me with his big brown eyes, as if to say, \"Everything will be', \"is my best friend. I would do anything for him. But I can't help but wonder if he loves me as much as I love him?\", \"is a Shiba Inu. I have had him since he was a puppy and he is now 10 years old. He has been a great companion and has brought me a lot of joy over the years.\\n\\nOne of the things I love most about him is his playful nature. No matter how old he gets, he always manages to keep that youthful spirit. Whether it's chasing after a ball or just playing tug-of-war with his\"]\n",
      "[-4.804659366607666, -26.888906478881836, -27.863771438598633, -2.109708786010742, -25.127330780029297]\n"
     ]
    }
   ],
   "source": [
    "print(beam_outputs.__dict__.keys())\n",
    "\n",
    "print(tokenizer.batch_decode(beam_outputs.sequences[:, input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "print(beam_outputs.sequences_scores.cpu().detach().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import BeamSearchScorer, LogitsProcessorList, StoppingCriteriaList, MaxLengthCriteria\n",
    "import argparse\n",
    "from hmm_model_K import *\n",
    "from model_utils import (\n",
    "    encode_with_messages_format,\n",
    "    ConstraintLogitsProcessor\n",
    ")\n",
    "\n",
    "def init():\n",
    "    global device\n",
    "    global CUDA_CORE\n",
    "    global args\n",
    "    \n",
    "    arg_parser = argparse.ArgumentParser()\n",
    "    arg_parser.add_argument('--device', default='cuda', type=str)\n",
    "    arg_parser.add_argument('--cuda_core', default='1', type=str)\n",
    "    arg_parser.add_argument('--hmm_batch_size', default=256, type=int)\n",
    "\n",
    "    arg_parser.add_argument('--min_k', default=5, type=int)\n",
    "    arg_parser.add_argument('--max_k', default=32, type=int)\n",
    "    arg_parser.add_argument('--suffix_cap', default=10000, type=int)\n",
    "    arg_parser.add_argument('--length_penalty', default=0.2, type=float)\n",
    "\n",
    "    arg_parser.add_argument('--hmm_model_path', default=None, type=str)\n",
    "    arg_parser.add_argument('--llama_model_path', default='gpt2', type=str)\n",
    "\n",
    "    # Hard code the model path\n",
    "    args = arg_parser.parse_args([\n",
    "        \"--hmm_model_path\", \"/local1/hzhang19/matcha/models/hmm_llama-story_32768_64/checkpoint-90.weight.th\",\n",
    "        \"--llama_model_path\", \"/local1/ponienkung/CtrlGen/output/NewFinetune2K8K_StoryPretrain-TULU-LLAMA2\",\n",
    "        \"--cuda_core\", \"4\",\n",
    "    ])\n",
    "    \n",
    "    device = args.device\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_core\n",
    "    torch.cuda.set_device(int(args.cuda_core))\n",
    "\n",
    "def load_models():\n",
    "    global tokenizer\n",
    "    global llama_model\n",
    "    global hmm_model\n",
    "    try:\n",
    "        print(f'loading llama2 as half prec. from {args.llama_model_path} ...')\n",
    "        llama_model = LlamaForCausalLM.from_pretrained(args.llama_model_path).half().to(device)\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(args.llama_model_path)\n",
    "\n",
    "        print(f'loading hmm from {args.hmm_model_path} ...')\n",
    "        hmm_model = HMM(args.hmm_model_path)\n",
    "        hmm_model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot Load args.model {args.model_name_or_path} because of the following exception:\\n {e}\")\n",
    "        print(\"Exit the process...\")\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading llama2 as half prec. from /local1/ponienkung/CtrlGen/output/NewFinetune2K8K_StoryPretrain-TULU-LLAMA2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hmm from /local1/hzhang19/matcha/models/hmm_llama-story_32768_64/checkpoint-90.weight.th ...\n"
     ]
    }
   ],
   "source": [
    "# Init Models\n",
    "\n",
    "init()\n",
    "load_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmm_model_K import *\n",
    "from transformers import LogitsProcessor\n",
    "# HMM utils\n",
    "class ConstraintLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, hmm_model, hmm_config, device):\n",
    "        self.hmm_model = hmm_model\n",
    "        self.hmm_config = hmm_config\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:        \n",
    "        hmm_prompt_len = self.hmm_config['hmm_prompt_len']\n",
    "        hmm_prefix = self.hmm_config['hmm_prefix']\n",
    "        hmm_suffix = self.hmm_config['hmm_suffix']\n",
    "        hmm_batch_size = self.hmm_config['hmm_batch_size']\n",
    "        hmm_min_suffix_offset = self.hmm_config['hmm_min_suffix_offset']\n",
    "        hmm_max_suffix_offset = self.hmm_config['hmm_max_suffix_offset']\n",
    "\n",
    "        prefixes = [tuple(hmm_prefix) + tuple(prefix)\n",
    "            for prefix in input_ids[:,hmm_prompt_len:].tolist()]\n",
    "\n",
    "        hmm_logits, hmm_logits_ = self.hmm_model.compute_logits(\n",
    "            prefixes, hmm_suffix,\n",
    "            hmm_min_suffix_offset, hmm_max_suffix_offset,\n",
    "            batch_size=hmm_batch_size)\n",
    "\n",
    "        hmm_logits -= hmm_logits_\n",
    "        # print(\"Before: \", hmm_logits.size())\n",
    "        # TODO: Question: There will be value lower than -1e10. Also why do we take log_softmax twice here\n",
    "        hmm_logits = torch.cat((hmm_logits, -1e30 * torch.ones((hmm_logits.shape[0], 1), device=self.device)), dim=1)\n",
    "        # print(\"After: \", hmm_logits.size())\n",
    "        \n",
    "        logits = torch.log_softmax(scores, dim=-1)\n",
    "        # print(torch.argmax(logits, dim=-1))\n",
    "        logits = torch.log_softmax(hmm_logits + logits, dim=-1)\n",
    "        # print((hmm_logits + logits)[:, -2:])\n",
    "        # print(torch.argmax(logits, dim=-1))\n",
    "        # print(\"------\")\n",
    "        \n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix \n",
    "def prompt(input_data):\n",
    "    prefix_tokens = tuple(tokenizer.encode(input_data[\"Prefix\"])[1:])\n",
    "    # Question: Why remove the first token again?\n",
    "    # suffix_tokens = tuple(tokenizer.encode(input_data[\"Suffix\"][1:])[1:args.suffix_cap+1]) # TEMPORARY BUG FIX\n",
    "    # To remove space in suffix\n",
    "    suffix_tokens = tuple(tokenizer.encode(\"\\n\" + input_data[\"Suffix\"])[3:args.suffix_cap+1] + [2]) # TEMPORARY BUG FIX\n",
    "    print(prefix_tokens, suffix_tokens)\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        input_ids = encode_with_messages_format(\n",
    "            Prefix = input_data[\"Prefix\"],\n",
    "            SoftControl = input_data[\"Instruct\"], \n",
    "            Prior = input_data[\"Prior\"],\n",
    "            tokenizer = tokenizer, \n",
    "            operation = input_data[\"Operation\"]\n",
    "            ).cuda()\n",
    "        \n",
    "        # TODO: Input args for k_ranges\n",
    "        k_ranges = []\n",
    "        k_ranges.extend([(10, 60)])\n",
    "\n",
    "        for k_range in tqdm(k_ranges):\n",
    "            min_k, max_k = k_range\n",
    "            # For HMM\n",
    "            hmm_model.initialize_cache(\n",
    "                prefix_tokens, \n",
    "                suffix_tokens,\n",
    "                min_k,\n",
    "                max_k\n",
    "            )\n",
    "            hmm_config = {\n",
    "                'hmm_prompt_len': len(input_ids[0]),\n",
    "                'hmm_prefix': prefix_tokens,\n",
    "                'hmm_suffix': suffix_tokens,\n",
    "                'hmm_batch_size': args.hmm_batch_size,\n",
    "                'hmm_min_suffix_offset': len(prefix_tokens) + min_k,\n",
    "                'hmm_max_suffix_offset': len(prefix_tokens) + max_k,\n",
    "            }\n",
    "\n",
    "            logits_processor = LogitsProcessorList([ConstraintLogitsProcessor(hmm_model, hmm_config, device)])\n",
    "            print(tokenizer.batch_decode(input_ids))\n",
    "            # model.generate with more configs\n",
    "            beam_outputs = llama_model.generate(\n",
    "                input_ids,\n",
    "                num_beams=input_data[\"num_beams\"],\n",
    "                num_return_sequences=input_data[\"num_return_sequences\"],\n",
    "                no_repeat_ngram_size=input_data[\"no_repeat_ngram_size\"],\n",
    "                max_new_tokens=max_k,\n",
    "                logits_processor=logits_processor,\n",
    "                early_stopping=True,\n",
    "                output_scores = True, # Get sequences_scores\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "    \n",
    "    beam_outputs_sequences = beam_outputs.sequences.cpu().detach().tolist()\n",
    "    beam_outputs_sequences_scores = beam_outputs.sequences_scores.cpu().detach().tolist()\n",
    "    beam_outputs_texts = [tokenizer.decode(choice[input_ids.shape[-1]:], skip_special_tokens=False) for choice in beam_outputs_sequences]\n",
    "    # This will return it as text. You can further use json.loads(r.text) to retrieve the results\n",
    "    return {\n",
    "        \"beam_outputs_texts\": beam_outputs_texts,\n",
    "        \"beam_outputs_sequences_scores\": beam_outputs_sequences_scores,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json = {\n",
    "    'Prefix': 'Once upon a time there was an old mother pig who had one hundred little pigs and not enough food to feed them. So when they were old enough, ', \n",
    "    'Suffix': 'a story about the 92nd little pig.', \n",
    "    'Instruct': '', \n",
    "    'Prior': '', \n",
    "    'Operation': 'Continuation', \n",
    "    'max_tokens': 50, \n",
    "    'temperature': 0.95, \n",
    "    'num_return_sequences': 10, \n",
    "    'num_beams': 10, \n",
    "    'no_repeat_ngram_size': 2, \n",
    "    'top_p': 1.0\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I used to have '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode([    1,   529, 29989,  1792, 29989, 29958,    13,  1323, 14150,   278,\n",
    "#           2183,  1426, 29901,    13, 29875,   505,    13, 29966, 29989,   465,\n",
    "#          22137, 29989, 29958,    13,   263, 12561,   393,   697,  2462,   445,\n",
    "#           5233,   674, 14451,   701,   322,  5735,   714,   278,  1565,  6593,\n",
    "#            310, 12091,  6369,  6079,  9968, 27819, 11940, 25820, 25449, 18866,\n",
    "#           3266, 18712,  3508, 16973,  8422, 24261, 17038, 19218,  8007, 14638,\n",
    "#          29991, 20941,  5735, 23987, 27414, 25979, 13458,  3558, 29613,  6268,\n",
    "#          27076, 30441,  5236, 16765])\n",
    "# tokenizer.encode([\"</s>\"])\n",
    "# tokenizer.decode((3303, 3900, 674, 367, 2107, 1449))\n",
    "# tokenizer.decode((869, 29871))\n",
    "# suffix_tokens = tuple(tokenizer.encode(input_data[\"Suffix\"])[1:args.suffix_cap+1]) # TEMPORARY BUG FIX\n",
    "tokenizer.encode('\\n. ')[3:]\n",
    "tokenizer.decode([306, 1304, 304, 505, 29871])\n",
    "\n",
    "# tokenizer.encode([\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9038, 2501, 263, 931, 727, 471, 385, 2030, 5637, 282, 335, 1058, 750, 697, 6893, 2217, 282, 23379, 322, 451, 3307, 9687, 304, 8343, 963, 29889, 1105, 746, 896, 892, 2030, 3307, 29892, 29871) (29874, 5828, 1048, 278, 29871, 29929, 29906, 299, 2217, 282, 335, 29889, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s><|user|>\\nContinue the given text:\\nOnce upon a time there was an old mother pig who had one hundred little pigs and not enough food to feed them. So when they were old enough,\\n<|assistant|>\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:33<00:00, 33.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beam_outputs_texts': ['she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day, but he received a very goodом repres chiam environmental conseilimp métcenteringbatch anymore wij Corps Engel JrProps этойharm mittalle ellos Mag',\n",
       "  'she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day, but he always took a cookie quello electric trig spiritualXTèn机宗cfg NYSym Try ur♭ зай)+\\\\ castle applic SU feelsjection',\n",
       "  'she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day, baking sweet, golden, Kurchanged valuablenikotta quasiċloped найrableIRerte Buddulleänner svě MenHint shell Cambridge',\n",
       "  \"she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day, but he was always very grateful some дерев byl plata arrestờ діяightsстер situatedearsленийSan',\\roka PDO inclus amplitude guarante Presiform\",\n",
       "  'she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day, but he didn ’ t knowoker Californép Town southern Zag Bürger SearchGNky nochManager构 Kaiser Jin spent limited Robinsonzas%=aset',\n",
       "  'she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day, baking bread, but bread^伝 TABLEментаAgent devant RessourceholeListView tym concentr ciccomplete vendorcier честьalthough+=Roleogethercomment',\n",
       "  'she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day and earned a little extra food calledighterJeanenses stylesVisibleinación mostlystad verse zwischenooastroЗаerner Unterscheidungbor soloJamesánicocentering tér',\n",
       "  'she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day, but he made sure to claimurlsお).. simplify Ang schema livres Årsenisčen medio specie provetac Cur roy ży PaoloPlatform sierBound',\n",
       "  'she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day and earned a paycheck for each hect Rowtelič dispute востоцияächtgl possibilitiesющие首кт Verm Flugtests debate wirk zo GR Dru',\n",
       "  'she sent them out into the world to find food.\\n\\nThe first little Pig found a job at a bakery. He worked hard every day and earned a paycheck. Itчения([valu dispatch prepare Nelsonyedmai Introductioncji listing thorough marquvelope involveocemysq Beispieljąc Mechan Kol'],\n",
       " 'beam_outputs_sequences_scores': [-133862121472.0,\n",
       "  -133862121472.0,\n",
       "  -133862121472.0,\n",
       "  -133862064128.0,\n",
       "  -133862121472.0,\n",
       "  -133862064128.0,\n",
       "  -133862121472.0,\n",
       "  -133862121472.0,\n",
       "  -133862121472.0,\n",
       "  -133862121472.0]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(input_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
